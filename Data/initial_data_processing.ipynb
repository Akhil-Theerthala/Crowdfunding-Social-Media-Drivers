{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Introduction\n",
    "\n",
    "This is the 2023 revison of the **Social Media impact on the Crowdfunding Success** project. In this revison, there are a few changes made to the project. The changes are listed [here](https://www.evernote.com/shard/s458/sh/5feb1b0b-d2b6-d0d2-ce9e-1d65a87ebb81/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_name</th>\n",
       "      <th>page_created</th>\n",
       "      <th>likes_at_posting</th>\n",
       "      <th>followers_at_posting</th>\n",
       "      <th>post_created</th>\n",
       "      <th>post_created_date</th>\n",
       "      <th>post_created_time</th>\n",
       "      <th>type</th>\n",
       "      <th>total_interactions</th>\n",
       "      <th>likes</th>\n",
       "      <th>...</th>\n",
       "      <th>url</th>\n",
       "      <th>message</th>\n",
       "      <th>link</th>\n",
       "      <th>final_link</th>\n",
       "      <th>image_text</th>\n",
       "      <th>link_text</th>\n",
       "      <th>description</th>\n",
       "      <th>sponsor_id</th>\n",
       "      <th>sponsor_name</th>\n",
       "      <th>sponsor_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoFundMe</td>\n",
       "      <td>10-02-2010 02:10</td>\n",
       "      <td>1839689</td>\n",
       "      <td>1942958.0</td>\n",
       "      <td>2022-07-22 01:22:32 IST</td>\n",
       "      <td>22-07-2022</td>\n",
       "      <td>01:22:32</td>\n",
       "      <td>Photo</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.facebook.com/299947508549/posts/10...</td>\n",
       "      <td>“I knew I had to do this with as many veterans...</td>\n",
       "      <td>https://www.facebook.com/gofundme/photos/a.101...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HONOR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kickstarter</td>\n",
       "      <td>21-04-2009 16:42</td>\n",
       "      <td>1589842</td>\n",
       "      <td>1587991.0</td>\n",
       "      <td>2022-07-22 01:18:36 IST</td>\n",
       "      <td>22-07-2022</td>\n",
       "      <td>01:18:36</td>\n",
       "      <td>Link</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.facebook.com/73182029884/posts/101...</td>\n",
       "      <td>Artist and designer Janos Stone is creating a ...</td>\n",
       "      <td>https://www.kickstarter.com/projects/haus/toyh...</td>\n",
       "      <td>https://www.kickstarter.com/projects/haus/toyh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ToyHaus: a uniquely beautiful mini-playhouse f...</td>\n",
       "      <td>ToyHaus is a washable, popup mini-playhouse wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kickstarter</td>\n",
       "      <td>21-04-2009 16:42</td>\n",
       "      <td>1589842</td>\n",
       "      <td>1587991.0</td>\n",
       "      <td>2022-07-21 18:52:39 IST</td>\n",
       "      <td>21-07-2022</td>\n",
       "      <td>18:52:39</td>\n",
       "      <td>Link</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.facebook.com/73182029884/posts/101...</td>\n",
       "      <td>The inaugural meeting of Kickstarter’s Communi...</td>\n",
       "      <td>https://www.kickstarter.com/blog/reporting-bac...</td>\n",
       "      <td>https://www.kickstarter.com/blog/reporting-bac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reporting Back from the First Meeting of the K...</td>\n",
       "      <td>Earlier this month, I was honored to host the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GoFundMe</td>\n",
       "      <td>10-02-2010 02:10</td>\n",
       "      <td>1839047</td>\n",
       "      <td>1942443.0</td>\n",
       "      <td>2022-07-21 00:23:03 IST</td>\n",
       "      <td>21-07-2022</td>\n",
       "      <td>00:23:03</td>\n",
       "      <td>Native Video</td>\n",
       "      <td>163</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.facebook.com/299947508549/posts/10...</td>\n",
       "      <td>This 25-year-old man rushed into a burning hou...</td>\n",
       "      <td>https://www.facebook.com/peoplemag/videos/2043...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>People</td>\n",
       "      <td>\"Nicholas Bostic's heroic actions saved lives,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kickstarter</td>\n",
       "      <td>21-04-2009 16:42</td>\n",
       "      <td>1589866</td>\n",
       "      <td>1588006.0</td>\n",
       "      <td>2022-07-20 02:38:36 IST</td>\n",
       "      <td>20-07-2022</td>\n",
       "      <td>02:38:36</td>\n",
       "      <td>Link</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.facebook.com/73182029884/posts/101...</td>\n",
       "      <td>An occult-themed JRPG with a fully integrated ...</td>\n",
       "      <td>https://www.kickstarter.com/projects/cherrymoc...</td>\n",
       "      <td>https://www.kickstarter.com/projects/cherrymoc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXIT VEIL: Occult &amp; Tarot JRPG</td>\n",
       "      <td>A Dark-Psychedelic JRPG &amp; fully integrated Tar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page_name      page_created  likes_at_posting  followers_at_posting  \\\n",
       "0     GoFundMe  10-02-2010 02:10           1839689             1942958.0   \n",
       "1  Kickstarter  21-04-2009 16:42           1589842             1587991.0   \n",
       "2  Kickstarter  21-04-2009 16:42           1589842             1587991.0   \n",
       "3     GoFundMe  10-02-2010 02:10           1839047             1942443.0   \n",
       "4  Kickstarter  21-04-2009 16:42           1589866             1588006.0   \n",
       "\n",
       "              post_created post_created_date post_created_time          type  \\\n",
       "0  2022-07-22 01:22:32 IST        22-07-2022          01:22:32         Photo   \n",
       "1  2022-07-22 01:18:36 IST        22-07-2022          01:18:36          Link   \n",
       "2  2022-07-21 18:52:39 IST        21-07-2022          18:52:39          Link   \n",
       "3  2022-07-21 00:23:03 IST        21-07-2022          00:23:03  Native Video   \n",
       "4  2022-07-20 02:38:36 IST        20-07-2022          02:38:36          Link   \n",
       "\n",
       "  total_interactions  likes  ...  \\\n",
       "0                 21      9  ...   \n",
       "1                  3      1  ...   \n",
       "2                  9      4  ...   \n",
       "3                163     42  ...   \n",
       "4                  7      5  ...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.facebook.com/299947508549/posts/10...   \n",
       "1  https://www.facebook.com/73182029884/posts/101...   \n",
       "2  https://www.facebook.com/73182029884/posts/101...   \n",
       "3  https://www.facebook.com/299947508549/posts/10...   \n",
       "4  https://www.facebook.com/73182029884/posts/101...   \n",
       "\n",
       "                                             message  \\\n",
       "0  “I knew I had to do this with as many veterans...   \n",
       "1  Artist and designer Janos Stone is creating a ...   \n",
       "2  The inaugural meeting of Kickstarter’s Communi...   \n",
       "3  This 25-year-old man rushed into a burning hou...   \n",
       "4  An occult-themed JRPG with a fully integrated ...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.facebook.com/gofundme/photos/a.101...   \n",
       "1  https://www.kickstarter.com/projects/haus/toyh...   \n",
       "2  https://www.kickstarter.com/blog/reporting-bac...   \n",
       "3  https://www.facebook.com/peoplemag/videos/2043...   \n",
       "4  https://www.kickstarter.com/projects/cherrymoc...   \n",
       "\n",
       "                                          final_link  image_text  \\\n",
       "0                                                NaN       HONOR   \n",
       "1  https://www.kickstarter.com/projects/haus/toyh...         NaN   \n",
       "2  https://www.kickstarter.com/blog/reporting-bac...         NaN   \n",
       "3                                                NaN         NaN   \n",
       "4  https://www.kickstarter.com/projects/cherrymoc...         NaN   \n",
       "\n",
       "                                           link_text  \\\n",
       "0                                                NaN   \n",
       "1  ToyHaus: a uniquely beautiful mini-playhouse f...   \n",
       "2  Reporting Back from the First Meeting of the K...   \n",
       "3                                             People   \n",
       "4                     EXIT VEIL: Occult & Tarot JRPG   \n",
       "\n",
       "                                         description  sponsor_id sponsor_name  \\\n",
       "0                                                NaN         NaN          NaN   \n",
       "1  ToyHaus is a washable, popup mini-playhouse wi...         NaN          NaN   \n",
       "2  Earlier this month, I was honored to host the ...         NaN          NaN   \n",
       "3  \"Nicholas Bostic's heroic actions saved lives,...         NaN          NaN   \n",
       "4  A Dark-Psychedelic JRPG & fully integrated Tar...         NaN          NaN   \n",
       "\n",
       "  sponsor_category  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import json\n",
    "import spacy\n",
    "import random\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models \n",
    "import warnings\n",
    "import torch\n",
    "import joblib\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    pd.set_option('mode.chained_assignment', None)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "current_dir = r'/home/theerthala/Documents/repos/Crowdfunding-Social-Media-Drivers/Data/Original_dataset/'\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "stop = stopwords.words('english')\n",
    "post_data = pd.read_csv(current_dir+\"2022-07-22-09-31-10-IST-feed-download.csv\")\n",
    "\n",
    "\n",
    "#basic data cleaning\n",
    "post_data.drop_duplicates(subset=['Message'], keep='last', inplace=True)\n",
    "post_data.reset_index(inplace=True, drop=True)\n",
    "post_data.columns = [x.lower().replace(' ','_') for x in post_data.columns.to_list()]\n",
    "post_data.drop(['user_name', 'facebook_id', 'page_category', 'page_admin_top_country',\n",
    "                'overperforming_score_(weighted__—__likes_1x_shares_1x_comments_1x_love_1x_wow_1x_haha_1x_sad_1x_angry_1x_care_1x_)'],\n",
    "                 axis=1, inplace=True)\n",
    "post_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pre_processing and EDA\n",
    "\n",
    "* Entity Recognition\n",
    "* Stopword Removal\n",
    "* Lemmatization\n",
    "* Sponsor effect investigation\n",
    "* Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8786/8786 [00:00<00:00, 136721.73it/s]\n"
     ]
    }
   ],
   "source": [
    "text_columns = set(['message', 'image_text','link_text','description'])\n",
    "for col in text_columns:\n",
    "    post_data[col] = post_data[col].str.lower()\n",
    "    post_data[col] = post_data[col].str.replace(r'http?.*.com?', ' ', regex=True)\n",
    "    post_data[col] = post_data[col].str.replace('[^\\w\\s]','', regex=True)\n",
    "\n",
    "# Extracting NERs out of the messages and other text bodies\n",
    "def recognise_entities(col, entities):\n",
    "    null_count = 0\n",
    "    for i in tqdm(range(len(post_data))):\n",
    "        try:\n",
    "            doc = nlp(post_data[col][i])\n",
    "            entities.append((i, [(ent.text, ent.label_) for ent in doc.ents]))\n",
    "        except (TypeError, ValueError):\n",
    "            null_count += 1\n",
    "    \n",
    "    print(f\"Number of null values in {col}: {null_count}\")\n",
    "    return entities\n",
    "    \n",
    "        \n",
    "def get_entities(current_dir, text_columns):\n",
    "    if os.path.isfile(current_dir+'idx_to_list.json'):\n",
    "        with open(current_dir+'idx_to_list.json', 'r') as f:\n",
    "            idx_to_list = json.load(f)\n",
    "    \n",
    "    else:\n",
    "        if os.path.isfile(current_dir+'entities.json'):\n",
    "            with open(current_dir+'entities.json', 'r') as f:\n",
    "                entities = json.load(f)\n",
    "        else:\n",
    "            entities=dict()\n",
    "            for text_col in text_columns:\n",
    "                temp_entities = []\n",
    "                entities[text_col] = recognise_entities(text_col, temp_entities)\n",
    "\n",
    "        indices = []\n",
    "\n",
    "        for key in entities.keys():\n",
    "            indices+= [x[0] for x in entities[key]]\n",
    "\n",
    "        indices = list(set(indices))\n",
    "\n",
    "        idx_to_list = dict()\n",
    "        for key in entities.keys():\n",
    "            for idx, recognised_entity_list in entities[key]:\n",
    "                if idx in idx_to_list.keys():\n",
    "                    idx_to_list[idx] += (recognised_entity_list)\n",
    "                else:\n",
    "                    idx_to_list[idx] = recognised_entity_list\n",
    "                idx_to_list[idx] = list(set(idx_to_list[idx]))\n",
    "\n",
    "            \n",
    "    return idx_to_list\n",
    "\n",
    "\n",
    "all_entities = get_entities(current_dir, text_columns)\n",
    "indices = sorted([int(key) for key in all_entities.keys()])\n",
    "all_entity_types = [[y for x, y in entity] for entity in [all_entities[str(idx)] for idx in indices]]\n",
    "\n",
    "set_all_entities = []\n",
    "[set_all_entities.extend(x) for x in all_entity_types]\n",
    "\n",
    "unique_entity_types = list(set(set_all_entities))\n",
    "entity_type_count = [(x, set_all_entities.count(x)) for x in unique_entity_types]\n",
    "entity_type_count.sort(key=lambda x: x[1], reverse=True)    \n",
    "post_data.loc[:, 'entities_identified'] = pd.Series(all_entity_types, index=indices)\n",
    "post_data.loc[:, 'entities_identified'] = post_data['entities_identified'].apply(lambda x: list(set(x)))\n",
    "entity_counts = post_data['entities_identified'].explode().value_counts()\n",
    "sorted_entities = entity_counts.index.to_list()\n",
    "\n",
    "# print(\"Entities identified: \", len(sorted_entities))\n",
    "\n",
    "# Making sure that the data doesn't have null values in the empty columns\n",
    "post_data.loc[:, list(text_columns)] = post_data.loc[:, list(text_columns)].fillna(' ')\n",
    "# [post_data[x].isnull().sum() for x in text_columns]\n",
    "\n",
    "post_data.loc[:, list(text_columns)]\n",
    "\n",
    "# removing stopwords and links in the texts\n",
    "for text_col in text_columns:\n",
    "    post_data[text_col] = post_data[text_col].apply(lambda x: ' '.join([word.strip() for word in x.split() if word not in (stop)]))\n",
    "\n",
    "def lemmatize_text(col):\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "    processed_col = post_data[col].apply(nlp)\n",
    "    processed_col = processed_col.apply(lambda x: [token.lemma_ for token in x if token.lemma_ not in stop])\n",
    "    processed_col = processed_col.apply(lambda x: ' '.join(list(set(x))))\n",
    "    return processed_col\n",
    "\n",
    "#If lemmatized data is not available, then start lemmatization.\n",
    "if os.path.isfile(current_dir+'temp_lemmatized_text.csv'):\n",
    "    post_data.loc[:, list(text_columns)]= pd.read_csv(current_dir+'temp_lemmatized_text.csv')\n",
    "else:\n",
    "    for text_col in text_columns:\n",
    "        post_data[text_col] = lemmatize_text(text_col)\n",
    "\n",
    "    post_data.loc[:, list(text_columns)].to_csv('temp_lemmatized_text.csv', index=False)\n",
    "\n",
    "\n",
    "# Creating a column for the number of entities identified in each post\n",
    "post_data['post_sponsored'] = (post_data.sponsor_id.notnull()).astype(int)\n",
    "\n",
    "#Clipping the dataset.\n",
    "max_likes = round(np.quantile(post_data.likes, 0.99))\n",
    "max_comments = round(np.quantile(post_data.comments, 0.99))\n",
    "max_shares = round(np.quantile(post_data.shares, 0.99))\n",
    "\n",
    "post_data = post_data[post_data.likes<max_likes].reset_index(drop=True)\n",
    "post_data = post_data[post_data.comments<max_comments].reset_index(drop=True)\n",
    "post_data = post_data[post_data.shares<max_shares].reset_index(drop=True)\n",
    "\n",
    "#Calculating Age of the post and the page\n",
    "post_data.page_created = pd.to_datetime(post_data.page_created, dayfirst=True)\n",
    "post_data.post_created = pd.to_datetime(post_data['post_created'].str.replace('IST', ''))\n",
    "post_data['current_date'] = pd.to_datetime('2022-07-22 09:31:10') \n",
    "post_data['post_age'] = (post_data.current_date - post_data.post_created).dt.days\n",
    "post_data['page_age'] = (post_data.current_date - post_data.page_created).dt.days\n",
    "\n",
    "#grouping public engagements into two broad categories\n",
    "post_data['positive_reactions'] = post_data['love'] + post_data['haha'] + post_data['wow'] + post_data['care']\n",
    "post_data['negative_reactions'] = post_data['sad'] + post_data['angry']\n",
    "post_data['emoji_reactions'] = post_data['positive_reactions'] + post_data['negative_reactions'] + post_data['likes']\n",
    "\n",
    "#Calculating overall views\n",
    "zero_view_indices = post_data[post_data.post_views ==0].index.to_list()\n",
    "non_zero_view_indices = post_data[post_data.post_views !=0].index.to_list()\n",
    "\n",
    "post_data.loc[zero_view_indices, 'post_views'] = post_data.loc[zero_view_indices, ['emoji_reactions', 'comments', 'shares', 'total_views', 'total_views_for_all_crossposts']].max(axis=1).astype(int)\n",
    "post_data.loc[non_zero_view_indices, 'post_views'] = post_data.loc[non_zero_view_indices, ['post_views', 'emoji_reactions', 'comments', 'shares', 'total_views', 'total_views_for_all_crossposts']].max(axis=1).astype(int)\n",
    "\n",
    "#filling followers_at_posting null values\n",
    "likes_and_following = post_data[[ 'likes_at_posting', 'followers_at_posting']]\n",
    "null_indices = likes_and_following[likes_and_following.followers_at_posting.isnull()].index.to_list()\n",
    "\n",
    "\n",
    "#loading the model\n",
    "with open(current_dir+'followers_model.pkl', 'rb') as f:\n",
    "    lr = joblib.load(f)\n",
    "\n",
    "needed = likes_and_following.loc[null_indices, 'likes_at_posting'].values.reshape(-1, 1)\n",
    "predicted = lr.predict(needed)\n",
    "likes_and_following.loc[null_indices, 'followers_at_posting'] = predicted\n",
    "post_data['followers_at_posting'] = likes_and_following['followers_at_posting'].astype(int)\n",
    "\n",
    "#Cleaning the `type` column\n",
    "video_types = ['Native Video', 'Video', 'Live Video Complete', 'Status', 'Live Video Scheduled', 'YouTube']\n",
    "post_data.loc[post_data['type'].isin(video_types), 'type'] = 'Video'\n",
    "post_data.loc[:, 'type'] = post_data['type'].str.lower()\n",
    "\n",
    "#Combining the text columns into a single column\n",
    "post_data.loc[:, list(text_columns)] = post_data.loc[:, list(text_columns)].fillna('')\n",
    "post_data['post_text'] = post_data['message'] + ' ' + post_data['image_text'] + ' ' + post_data['link_text'] + ' ' + post_data['description']\n",
    "post_data['post_text'] = post_data.post_text.str.split().apply(lambda x: [word.strip() for word in x])\n",
    "\n",
    "\n",
    "#Cleaning the rare_words\n",
    "all_text = []\n",
    "for i in tqdm(range(len(post_data))):\n",
    "    all_text.extend(post_data.post_text[i])\n",
    "\n",
    "vocab = set(all_text)\n",
    "word_freq = nltk.FreqDist(all_text)\n",
    "rare_words = [word for word in vocab if word_freq[word] < 4]\n",
    "too_frequent_words = [word for word in vocab if word_freq[word] > 155]\n",
    "words_to_remove = rare_words + too_frequent_words\n",
    "\n",
    "post_data['post_text'] = post_data['post_text'].apply(lambda x: [word for word in x if word not in words_to_remove]) \n",
    "\n",
    "\n",
    "post_data.reset_index(drop=True, inplace=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropping_cols = ['sponsor_id', 'sponsor_name', 'sponsor_category', 'page_created', 'post_created', 'current_date',\n",
    "                 'total_interactions', 'post_created_date', 'post_created_time', 'love', 'haha', 'wow', 'sad', 'angry',\n",
    "                 'care',  'total_views', 'total_views_for_all_crossposts', 'url', 'link', 'final_link', \n",
    "                 'video_share_status', 'is_video_owner?', 'video_length', 'message', 'image_text', 'link_text', 'description',\n",
    "                 ]\n",
    "\n",
    "post_data.drop(dropping_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inprogress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['page_name', 'likes_at_posting', 'followers_at_posting', 'type',\n",
       "       'likes', 'comments', 'shares', 'post_views', 'entities_identified',\n",
       "       'post_sponsored', 'post_age', 'page_age', 'positive_reactions',\n",
       "       'negative_reactions', 'emoji_reactions', 'post_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [07:19<00:00, 39.92s/it]\n"
     ]
    }
   ],
   "source": [
    "texts = list(post_data.post_text)\n",
    "id2word = gensim.corpora.Dictionary(post_data.post_text)\n",
    "corpus = [id2word.doc2bow(doc) for doc in post_data.post_text]\n",
    "\n",
    "scores=[]\n",
    "\n",
    "def topic_diversity(ldamodel, topn=25):\n",
    "    # Get the topn words for each topic\n",
    "    topic_words = [ldamodel.show_topic(topicid, topn) for topicid in range(ldamodel.num_topics)]\n",
    "    # Count the unique words across all topics\n",
    "    unique_words = len({word for topic in topic_words for word, _ in topic})\n",
    "    # Calculate the topic diversity\n",
    "    diversity = unique_words / (ldamodel.num_topics * topn)\n",
    "    return diversity\n",
    "\n",
    "\n",
    "for k in tqdm(range(4,15)): # Train LDA on different values of k\n",
    "    \n",
    "    ldamodel =gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=k, \n",
    "                                       random_state=42,\n",
    "                                       chunksize=100,\n",
    "                                       passes=100,\n",
    "                                       per_word_topics=True)\n",
    "    #Calculating the coherence\n",
    "    if not os.path.isdir(current_dir + f'lda_models/{k}_topics'):\n",
    "        os.mkdir(current_dir + f'lda_models/{k}_topics')\n",
    "\n",
    "    ldamodel.save(current_dir + f'lda_models/{k}_topics/lda_model_{k}_topics')\n",
    "\n",
    "\n",
    "    cm = gensim.models.coherencemodel.CoherenceModel(model=ldamodel, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "    scores.append((k, cm.get_coherence(), topic_diversity(ldamodel), ldamodel.log_perplexity(corpus)))\n",
    "\n",
    "\n",
    "lda_scores = pd.DataFrame(scores, columns=['num_topics', 'coherence', 'topic_diversity', 'perplexity'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crowdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
